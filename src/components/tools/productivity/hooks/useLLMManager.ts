import { useState, useCallback, useEffect } from 'react';
import { supabase } from '@/integrations/supabase/client';
import { useAuth } from '@/contexts/AuthContext';
import { useToast } from '@/hooks/use-toast';
import { AIJsonParser } from '../utils/aiJsonParser';

interface LLMProvider {
  id: string;
  provider: string;
  api_key: string;
  is_default: boolean;
  selected_model: string | null;
}

interface DecompositionRequest {
  taskTitle: string;
  taskDescription?: string;
  tags?: string[];
  priority?: 'low' | 'medium' | 'high';
  category?: string;
  estimatedDuration?: number;
  context?: string;
}

interface SubtaskData {
  title: string;
  description: string;
  estimatedDuration?: number;
  priority?: 'low' | 'medium' | 'high';
  order?: number;
}

interface DecompositionResult {
  success: boolean;
  subtasks: SubtaskData[];
  error?: string;
}

// Standalone function to generate emergency subtasks
const generateEmergencySubtasks = (request: DecompositionRequest): SubtaskData[] => {
  const baseTitle = request.taskTitle;
  const totalDuration = request.estimatedDuration || 120;
  const avgDuration = Math.round(totalDuration / 4);
  
  return [
    {
      title: `${baseTitle} - Phase 1: Analyse`,
      description: 'Analyser les exigences et d√©finir les objectifs',
      estimatedDuration: avgDuration,
      priority: 'high' as const,
      order: 1
    },
    {
      title: `${baseTitle} - Phase 2: Pr√©paration`,
      description: 'Pr√©parer les ressources et outils n√©cessaires',
      estimatedDuration: avgDuration,
      priority: 'medium' as const,
      order: 2
    },
    {
      title: `${baseTitle} - Phase 3: Ex√©cution`,
      description: 'R√©aliser le travail principal',
      estimatedDuration: avgDuration,
      priority: 'high' as const,
      order: 3
    },
    {
      title: `${baseTitle} - Phase 4: Finalisation`,
      description: 'R√©viser, tester et finaliser',
      estimatedDuration: avgDuration,
      priority: 'medium' as const,
      order: 4
    }
  ];
};

export const useLLMManager = () => {
  const { user } = useAuth();
  const { toast } = useToast();
  const [providers, setProviders] = useState<LLMProvider[]>([]);
  const [defaultProvider, setDefaultProvider] = useState<LLMProvider | null>(null);
  const [isLoading, setIsLoading] = useState(false);

  useEffect(() => {
    if (user) {
      loadProviders();
    }
  }, [user]);

  const loadProviders = useCallback(async () => {
    console.log('üîç [useLLMManager] D√©but du chargement des providers');
    try {
      const { data: { user } } = await supabase.auth.getUser();
      console.log('üîç [useLLMManager] Utilisateur:', user ? 'connect√©' : 'non connect√©');
      
      if (user) {
        const { data, error } = await supabase
          .from('user_llm_api_keys')
          .select('*')
          .eq('user_id', user.id);

        if (error) {
          console.error('‚ùå [useLLMManager] Erreur chargement providers Supabase:', error);
          // Fallback vers localStorage
          const localProviders = localStorage.getItem('llm_providers');
          console.log('üîç [useLLMManager] Fallback localStorage:', localProviders ? 'trouv√©' : 'vide');
          if (localProviders) {
            const parsedProviders = JSON.parse(localProviders);
            console.log('üîç [useLLMManager] Providers localStorage:', parsedProviders.length, 'providers');
            setProviders(parsedProviders);
            const defaultProv = parsedProviders.find((p: LLMProvider) => p.is_default);
            console.log('üîç [useLLMManager] Default provider localStorage:', defaultProv ? `${defaultProv.provider} (${defaultProv.selected_model})` : 'aucun');
            setDefaultProvider(defaultProv || null);
          }
          return;
        }

        if (data && data.length > 0) {
          console.log('‚úÖ [useLLMManager] Providers Supabase charg√©s:', data.length, 'providers');
          setProviders(data);
          const defaultProv = data.find(provider => provider.is_default);
          console.log('üîç [useLLMManager] Default provider Supabase:', defaultProv ? `${defaultProv.provider} (${defaultProv.selected_model})` : 'aucun');
          setDefaultProvider(defaultProv || null);
          // Sauvegarder en localStorage comme backup
          localStorage.setItem('llm_providers', JSON.stringify(data));
        } else {
          console.log('‚ö†Ô∏è [useLLMManager] Aucune donn√©e Supabase, essai localStorage');
          // Pas de donn√©es Supabase, essayer localStorage
          const localProviders = localStorage.getItem('llm_providers');
          if (localProviders) {
            const parsedProviders = JSON.parse(localProviders);
            console.log('üîç [useLLMManager] Providers localStorage (fallback):', parsedProviders.length, 'providers');
            setProviders(parsedProviders);
            const defaultProv = parsedProviders.find((p: LLMProvider) => p.is_default);
            console.log('üîç [useLLMManager] Default provider localStorage (fallback):', defaultProv ? `${defaultProv.provider} (${defaultProv.selected_model})` : 'aucun');
            setDefaultProvider(defaultProv || null);
          }
        }
      } else {
        console.log('üë§ [useLLMManager] Utilisateur non connect√©, utilisation localStorage');
        // Utilisateur non connect√©, utiliser localStorage
        const localProviders = localStorage.getItem('llm_providers');
        if (localProviders) {
          const parsedProviders = JSON.parse(localProviders);
          console.log('üîç [useLLMManager] Providers localStorage (non connect√©):', parsedProviders.length, 'providers');
          setProviders(parsedProviders);
          const defaultProv = parsedProviders.find((p: LLMProvider) => p.is_default);
          console.log('üîç [useLLMManager] Default provider localStorage (non connect√©):', defaultProv ? `${defaultProv.provider} (${defaultProv.selected_model})` : 'aucun');
          setDefaultProvider(defaultProv || null);
        } else {
          console.log('‚ö†Ô∏è [useLLMManager] Aucun provider en localStorage');
        }
      }
    } catch (error) {
      console.error('‚ùå [useLLMManager] Erreur chargement providers:', error);
      // Fallback vers localStorage en cas d'erreur
      const localProviders = localStorage.getItem('llm_providers');
      if (localProviders) {
        const parsedProviders = JSON.parse(localProviders);
        console.log('üîç [useLLMManager] Providers localStorage (erreur):', parsedProviders.length, 'providers');
        setProviders(parsedProviders);
        const defaultProv = parsedProviders.find((p: LLMProvider) => p.is_default);
        console.log('üîç [useLLMManager] Default provider localStorage (erreur):', defaultProv ? `${defaultProv.provider} (${defaultProv.selected_model})` : 'aucun');
        setDefaultProvider(defaultProv || null);
      }
    }
  }, []);

  const decomposeTaskWithAI = useCallback(async (
    request: DecompositionRequest
  ): Promise<DecompositionResult> => {
    if (!defaultProvider) {
      return {
        success: false,
        subtasks: [],
        error: 'Aucun fournisseur LLM configur√© par d√©faut'
      };
    }

    setIsLoading(true);
    try {
      // Prompt optimis√© pour une meilleure g√©n√©ration JSON
      const prompt = `Tu es un expert en d√©composition de t√¢ches. Tu DOIS respecter ces r√®gles ABSOLUMENT :

R√àGLES STRICTES :
1. Cr√©e EXACTEMENT entre 4 et 8 sous-t√¢ches (jamais moins, jamais plus)
2. R√©ponds UNIQUEMENT en JSON valide, AUCUN autre texte
3. Chaque sous-t√¢che doit √™tre sp√©cifique et actionnable
4. Ordonne logiquement les sous-t√¢ches

T√ÇCHE √Ä D√âCOMPOSER :
Titre: "${request.taskTitle}"
${request.taskDescription ? `Description: "${request.taskDescription}"` : ''}
${request.estimatedDuration ? `Dur√©e totale estim√©e: ${request.estimatedDuration} minutes` : ''}
${request.priority ? `Priorit√©: ${request.priority}` : ''}
${request.category ? `Cat√©gorie: ${request.category}` : ''}

INSTRUCTIONS :
- Divise cette t√¢che en √©tapes logiques et s√©quentielles
- Chaque √©tape doit prendre entre 15 et 90 minutes
- Assure-toi que toutes les √©tapes sont n√©cessaires pour accomplir la t√¢che principale

FORMAT JSON STRICT (r√©ponds UNIQUEMENT avec ce JSON, rien d'autre) :
{
  "subtasks": [
    {
      "title": "Titre pr√©cis de l'√©tape 1",
      "description": "Description d√©taill√©e des actions √† effectuer",
      "estimatedDuration": 45,
      "priority": "high",
      "order": 1
    },
    {
      "title": "Titre pr√©cis de l'√©tape 2", 
      "description": "Description d√©taill√©e des actions √† effectuer",
      "estimatedDuration": 30,
      "priority": "medium",
      "order": 2
    }
  ]
}

IMPORTANT: R√©ponds UNIQUEMENT avec le JSON, aucun texte avant ou apr√®s !`;

      let rawResult: string;

      // Appel √† l'API selon le fournisseur
      if (defaultProvider.provider === 'openai') {
        rawResult = await callOpenAI(defaultProvider.api_key, prompt, defaultProvider.selected_model);
      } else if (defaultProvider.provider === 'anthropic') {
        rawResult = await callAnthropic(defaultProvider.api_key, prompt, defaultProvider.selected_model);
      } else if (defaultProvider.provider === 'google') {
        rawResult = await callGoogle(defaultProvider.api_key, prompt, defaultProvider.selected_model);
      } else if (defaultProvider.provider === 'deepseek') {
        rawResult = await callDeepSeek(defaultProvider.api_key, prompt, defaultProvider.selected_model);
      } else if (defaultProvider.provider === 'openrouter') {
        rawResult = await callOpenRouter(defaultProvider.api_key, prompt, defaultProvider.selected_model);
      } else if (defaultProvider.provider === 'xgrok') {
        rawResult = await callXGrok(defaultProvider.api_key, prompt, defaultProvider.selected_model);
      } else {
        throw new Error(`Fournisseur ${defaultProvider.provider} non support√©`);
      }

      console.log('ü§ñ R√©ponse brute de l\'IA:', rawResult.substring(0, 300) + '...');

      // Utiliser le nouveau parseur robuste
      const parseResult = AIJsonParser.parseWithRecovery(rawResult);
      
      if (!parseResult.success) {
        throw new Error(parseResult.error || '√âchec du parsing JSON');
      }

      console.log(`‚úÖ ${parseResult.subtasks.length} sous-t√¢ches g√©n√©r√©es avec m√©thode: ${parseResult.recoveryMethod}`);

      // Enrichir les sous-t√¢ches avec les donn√©es de la requ√™te
      const enrichedSubtasks = parseResult.subtasks.map((subtask, index) => ({
        ...subtask,
        priority: subtask.priority || request.priority || 'medium',
        estimatedDuration: subtask.estimatedDuration || 
          (request.estimatedDuration ? Math.round(request.estimatedDuration / parseResult.subtasks.length) : 30),
        order: index + 1
      }));

      return {
        success: true,
        subtasks: enrichedSubtasks
      };
    } catch (error) {
      console.error('‚ùå Erreur d√©composition IA:', error);
      
      // G√©n√©ration de sous-t√¢ches de secours en utilisant la fonction standalone
      const fallbackSubtasks = generateEmergencySubtasks(request);
      
      return {
        success: true,
        subtasks: fallbackSubtasks,
        error: `Utilis√© les sous-t√¢ches de secours: ${error instanceof Error ? error.message : 'Erreur inconnue'}`
      };
    } finally {
      setIsLoading(false);
    }
  }, [defaultProvider]);

  const callOpenAI = async (apiKey: string, prompt: string, selectedModel?: string | null): Promise<string> => {
    const model = selectedModel || 'gpt-4o';
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        messages: [
          {
            role: 'system',
            content: 'Vous √™tes un assistant sp√©cialis√© dans la d√©composition de t√¢ches. R√©pondez uniquement en JSON valide.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 500,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(`Erreur OpenAI: ${errorData.error?.message || response.status}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || '';
  };

  const callAnthropic = async (apiKey: string, prompt: string, selectedModel?: string | null): Promise<string> => {
    const model = selectedModel || 'claude-3-5-sonnet-20241022';
    
    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'x-api-key': apiKey,
        'Content-Type': 'application/json',
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model,
        max_tokens: 500,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ],
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(`Erreur Anthropic: ${errorData.error?.message || response.status}`);
    }

    const data = await response.json();
    return data.content[0]?.text || '';
  };

  const callGoogle = async (apiKey: string, prompt: string, selectedModel?: string | null): Promise<string> => {
    const model = selectedModel || 'gemini-2.0-flash-exp';
    
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [
          {
            parts: [
              {
                text: prompt
              }
            ]
          }
        ],
        generationConfig: {
          maxOutputTokens: 500,
          temperature: 0.7,
        }
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(`Erreur Google: ${errorData.error?.message || response.status}`);
    }

    const data = await response.json();
    return data.candidates[0]?.content?.parts[0]?.text || '';
  };

  const callDeepSeek = async (apiKey: string, prompt: string, selectedModel?: string | null): Promise<string> => {
    const model = selectedModel || 'deepseek-chat';
    
    const response = await fetch('https://api.deepseek.com/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        messages: [
          {
            role: 'system',
            content: 'Vous √™tes un assistant sp√©cialis√© dans la d√©composition de t√¢ches. R√©pondez uniquement en JSON valide.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 500,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(`Erreur DeepSeek: ${errorData.error?.message || response.status}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || '';
  };

  const callOpenRouter = async (apiKey: string, prompt: string, selectedModel?: string | null): Promise<string> => {
    const model = selectedModel || 'deepseek/deepseek-r1';
    
    const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'HTTP-Referer': window.location.origin,
      },
      body: JSON.stringify({
        model,
        messages: [
          {
            role: 'system',
            content: 'Vous √™tes un assistant sp√©cialis√© dans la d√©composition de t√¢ches. R√©pondez uniquement en JSON valide.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 500,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(`Erreur OpenRouter: ${errorData.error?.message || response.status}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || '';
  };

  const callXGrok = async (apiKey: string, prompt: string, selectedModel?: string | null): Promise<string> => {
    const model = selectedModel || 'grok-beta';
    
    const response = await fetch('https://api.x.ai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model,
        messages: [
          {
            role: 'system',
            content: 'Vous √™tes un assistant sp√©cialis√© dans la d√©composition de t√¢ches. R√©pondez uniquement en JSON valide.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 500,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json();
      throw new Error(`Erreur X/Grok: ${errorData.error?.message || response.status}`);
    }

    const data = await response.json();
    return data.choices[0]?.message?.content || '';
  };

  // Debug logs pour hasConfiguredProvider avec validation API key
  const hasConfiguredProvider = !!defaultProvider && !!defaultProvider.api_key && defaultProvider.api_key.trim() !== '';
  console.log('üîç [useLLMManager] hasConfiguredProvider:', hasConfiguredProvider);
  console.log('üîç [useLLMManager] defaultProvider:', defaultProvider ? `${defaultProvider.provider} (${defaultProvider.selected_model})` : 'null');
  console.log('üîç [useLLMManager] defaultProvider API key:', defaultProvider?.api_key ? `${defaultProvider.api_key.substring(0, 10)}...` : 'null');
  console.log('üîç [useLLMManager] providers count:', providers.length);
  
  // Debug d√©taill√© de tous les providers
  providers.forEach((provider, index) => {
    console.log(`üîç [useLLMManager] Provider ${index + 1}:`, {
      id: provider.id,
      provider: provider.provider,
      hasApiKey: !!provider.api_key,
      apiKeyLength: provider.api_key?.length || 0,
      isDefault: provider.is_default,
      selectedModel: provider.selected_model
    });
  });

  return {
    providers,
    defaultProvider,
    decomposeTaskWithAI,
    isLoading,
    hasConfiguredProvider,
    reloadProviders: loadProviders,
  };
};
